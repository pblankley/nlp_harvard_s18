{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: PyTorch Tutorial (Tensors, Automatic Differentation, Linear Regression, MNIST Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "1. Learn to work with/manipulate tensors in PyTorch\n",
    "2. Learn about automatic differentiation\n",
    "3. Fit a linear regression model\n",
    "3. Build a simple classifier on MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors, or multidimensional arrays, are fundamental objects that you will be working with in Torch (and deep learning in general). Let's create some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(5,5) #create a 5 x 5 tensor\n",
    "print(x)\n",
    "x = torch.LongTensor(5,2) #often our input data consists of integers (word indices), so it's helpful to use LongTensors\n",
    "print(x)\n",
    "#we can also do other initializations\n",
    "x = torch.randn(3,4) #initialize from standard normal\n",
    "x = torch.ones(3,4) #all ones\n",
    "x = torch.zeros(3,4) #all zeros\n",
    "x = torch.eye(3) # identity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go back between numpy/torch tensors with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_numpy = np.random.randn(5)\n",
    "print(x_numpy)\n",
    "x_torch = torch.from_numpy(x_numpy)\n",
    "print(x_torch)\n",
    "x_numpy2 = x_torch.numpy()\n",
    "print(x_numpy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing torch tensors is essentially identitical to the case in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4,3)\n",
    "print(x[0, 0]) \n",
    "print(x[:, 0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways to manipulate tensors. Of particular utility is the *view* function, which reshapes a tensor in memory. See [here](http://pytorch.org/docs/master/tensors.html) for more operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.view(2,6)) #reshape the 4x3 tensor into a 2x6 tensor\n",
    "print(x.view(-1)) # -1 always reshapes to a 1d tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use operations on tensors as in numpy. Operations that have an underscore _ are *in-place* and modify the original tensor. Other operations will create a new tensor in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 5)\n",
    "y = torch.randn(5, 5)\n",
    "z = torch.mm(x, y) #matrix multiply\n",
    "z = 2*x # scalar multiplication\n",
    "z = x + y #addition, creates a new tensor\n",
    "print(x)\n",
    "z = x.add(y) #same\n",
    "print(x)\n",
    "x.add_(y) #modifies x by adding y to it\n",
    "print(x)\n",
    "#there are other operations such as x.mul_(), x.div_() etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so the tensor object is basically the same as a numpy array. What's neat is that you can define **computation graphs** with tensors and backpropagate gradients through this computational graph automatically. This is known as automatic differentation. To do this however, we need to use a `Variable` object which is basically a wrapper around a tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "x = Variable(torch.randn(5), requires_grad=True) #requires grad means that we want to calculate gradients\n",
    "print(x.data) #a tensor object\n",
    "print(x.grad) #the gradient. right now we haven't calculated any gradients so there is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(5), requires_grad=True) \n",
    "y = x.mul(2) #y is now a variable. almost any operation you can do on tensors you can do on Variables\n",
    "print(y)\n",
    "y.backward(Variable(torch.ones(y.size()))) #this is a tricky concept, but we essentially backprop a vector of ones\n",
    "#to simulate the calculation of dy[i] / dx for all i\n",
    "print(x.grad) #this should be a vector of 2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in practice, we almost always calculate the derivatve with respect to a **scalar**. In this case we can simply call `.backward()` without having to backpropagate a 1x1 vector of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.data.zero_() #gradients are always accumulated, so we need to zero them out manually\n",
    "y = x.mul(2).sum() # now y is a scalar. In most cases this would be your average loss\n",
    "y.backward() #this is equivalent to y.backward(Variable(torch.ones(y.size())))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a simple least squares model on a synthetic dataset with gradient descent. First let's generate synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_points = 1000\n",
    "num_features = 5\n",
    "w_star = torch.randn(num_features) #true weight vector\n",
    "x = torch.randn(num_points, num_features) #input data\n",
    "y = torch.mm(x, w_star.view(5, 1)) #torch.mm expects both inputs to be matrices so we cast w_star to be a column vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem has an analytic solution which we can calculate directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ols = torch.mm(torch.mm(torch.inverse(torch.mm(x.t(), x)), x.t()), y) #(X^T X)^{-1} X^T Y\n",
    "print(w_ols)\n",
    "print(w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's see if we can obtain (approximately) the same solution with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sgd = Variable(torch.randn(num_features), requires_grad=True) #randomly initialize\n",
    "x_sgd = Variable(x) #remember, we need to convert everything to Variables to work with automatic differentiation\n",
    "y_sgd = Variable(y) #we don't need to calculate gradients with respect to these guys\n",
    "\n",
    "num_iters = 50\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(num_iters):\n",
    "    if w_sgd.grad is not None:\n",
    "        w_sgd.grad.zero_() #gradients get accumulated so we have to manually zero them out\n",
    "    y_pred = torch.mm(x_sgd, w_sgd.unsqueeze(1)) #unsqueeze adds an extra dimension\n",
    "    error = (y_sgd - y_pred)**2\n",
    "    error_avg = error.mean()\n",
    "    if i % 10 == 0:\n",
    "        print(i, error_avg.data[0])\n",
    "    error_avg.backward()    \n",
    "    w_sgd.data = w_sgd.data - learning_rate*w_sgd.grad.data\n",
    "        \n",
    "print(w_sgd)\n",
    "print(w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that worked, but it's a bit annoying to separately define the weights as `Variables` and manually apply matrix-multiplies. Fortunately, torch provides an `nn` package which provides abstractions for almost all of the layers that we will use in the course. Let's see a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "lin = nn.Linear(num_features, 1, bias=False) #this defines a linear layer that goes from num_feature dimensions to 1\n",
    "print(lin.weight) #weight parameter of the linear layer. if bias = True, then we can access bias with lin.bias\n",
    "y_pred = lin(x_sgd) #lin(x_sgd) automatically calls forward on the input x_sgd\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of `nn` layers can be found [here](http://pytorch.org/docs/master/nn.html). You should try to familiarize yourself with pretty much all the `nn` layers. Torch also provides an `optim` package that makes parameter updates easier. We will be working with SGD in this tutorial, but other optimization algorithms (Adam, Adagrad, RMSProp, etc) can be found [here](http://pytorch.org/docs/master/optim.html). Let's try to fit the linear regression model with these abstractions now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(lin.parameters(), lr = learning_rate) #initialize with parameters\n",
    "for i in range(num_iters):\n",
    "    optimizer.zero_grad() #this will zero out the gradients of all your parameters\n",
    "    y_pred = lin(x_sgd)\n",
    "    error = (y_sgd - y_pred)**2\n",
    "    error_avg = error.mean()    \n",
    "    error_avg.backward()    \n",
    "    optimizer.step()    \n",
    "print(lin.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "val_dataset = datasets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = train_dataset[0]\n",
    "print(image.size())\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch provides a loader around tensor datasets so you can create/access mini-batches easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "for (image, label) in train_loader: #iterates through the dataset in mini-batches\n",
    "    print(image.size())\n",
    "    print(label.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we are ready to create our first model, which will be a simple multilayer perceptron. To do this, it helps to define a `class` with all the layers inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_layers = 1, input_dim = 28*28, output_dim = 10, hidden_dim = 10):\n",
    "        super(MLP, self).__init__()                \n",
    "        self.hidden_to_output = nn.Linear(hidden_dim, output_dim)\n",
    "        hidden_layers = []\n",
    "        for l in range(num_layers):\n",
    "            dim = input_dim if l == 0 else hidden_dim #first layer is input to hidden layer\n",
    "            hidden_layers.append(nn.Linear(dim, hidden_dim))\n",
    "            hidden_layers.append(nn.ReLU()) #let's work with relu nonlinearities for now\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers) #Sequential module will apply the layers in sequence\n",
    "        self.logsoftmax = nn.LogSoftmax() #softmax will turn the output into probabilities, but log is more convnient\n",
    "        \n",
    "    def forward(self, x): #MLP(x) is shorthand for MLP.forward(x)\n",
    "        x_flatten = x.view(x.size(0), -1) #need to flatten batch_size x 1 x 28 x 28 to batch_size x 28*28\n",
    "        out = self.hidden_layers(x_flatten)\n",
    "        out = self.hidden_to_output(out) #you can redefine variables\n",
    "        return self.logsoftmax(out)\n",
    "        \n",
    "mlp = MLP(num_layers = 1)\n",
    "print(mlp)\n",
    "for p in mlp.parameters(): #all the parameters that were defined inside the module can be accessed like this\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the network is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp(Variable(image))\n",
    "print(y_pred) #these will be log probabilities over each of the 10 classes\n",
    "print(y_pred[0].exp()) #let's make sure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also convenient to define a test function that we can call periodically to check performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() #this is the negative log-likelihood for multi-class classification\n",
    "\n",
    "def test(model, data):\n",
    "    correct = 0.\n",
    "    num_examples = 0.\n",
    "    nll = 0.\n",
    "    for (image, label) in data:\n",
    "        image, label = Variable(image), Variable(label) #annoying, but necessary        \n",
    "        y_pred = mlp(image)\n",
    "        nll_batch = criterion(y_pred, label)\n",
    "        nll += nll_batch.data[0] * image.size(0) #by default NLL is averaged over each batch\n",
    "        y_pred_max, y_pred_argmax = torch.max(y_pred, 1) #prediction is the argmax\n",
    "        correct += (y_pred_argmax.data == label.data).sum() \n",
    "        num_examples += image.size(0) \n",
    "    return nll/num_examples, correct/num_examples\n",
    "nll, accuracy = test(mlp, val_loader)\n",
    "print('Validation performance. NLL: %.4f, Accuracy: %.4f'% (nll, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MLP(num_layers = 1)\n",
    "optim = torch.optim.SGD(mlp.parameters(), lr =0.5)\n",
    "num_epochs = 20\n",
    "for e in range(num_epochs):\n",
    "    for (image, label) in train_loader:\n",
    "        optim.zero_grad()\n",
    "        image, label = Variable(image), Variable(label) \n",
    "        y_pred = mlp(image)\n",
    "        nll_batch = criterion(y_pred, label)    \n",
    "        nll_batch.backward()\n",
    "        optim.step()\n",
    "    nll_train, accuracy_train = test(mlp, train_loader) #you never wanna do this in practice, since this will take forever\n",
    "    nll_val, accuracy_val = test(mlp, val_loader)\n",
    "    print('Training performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_train, accuracy_train))\n",
    "    print('Validation performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_val, accuracy_val))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with more hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MLP(num_layers = 1, hidden_dim = 100)\n",
    "print(mlp)\n",
    "optim = torch.optim.SGD(mlp.parameters(), lr = 0.5)\n",
    "num_epochs = 20\n",
    "for e in range(num_epochs):\n",
    "    for (image, label) in train_loader:\n",
    "        optim.zero_grad()\n",
    "        image, label = Variable(image), Variable(label) \n",
    "        y_pred = mlp(image)\n",
    "        nll_batch = criterion(y_pred, label)    \n",
    "        nll_batch.backward()\n",
    "        optim.step()\n",
    "    nll_train, accuracy_train = test(mlp, train_loader) \n",
    "    nll_val, accuracy_val = test(mlp, val_loader)\n",
    "    print('Training performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_train, accuracy_train))\n",
    "    print('Validation performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_val, accuracy_val))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. For the rest of section, try implementing a ConvNet. You may need to use more sophisticated optimization algorithms (`optim.Adam(model.parameters(), lr = 0.001)` should work well enough for most models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
